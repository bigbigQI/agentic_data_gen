#!/usr/bin/env python3
"""
å·¥å…·è´¨é‡è¯„ä¼°è„šæœ¬
ä½¿ç”¨å¤šçº¿ç¨‹æ–¹å¼æ‰¹é‡è¯„ä¼°å·¥å…·è´¨é‡
"""

import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from config.settings import settings
from modules.domain_tool_generator.tool_designer import ToolDesigner
from utils.logger import setup_logger
from utils.file_manager import FileManager


def load_tools_from_file(file_path: str):
    """ä»æŒ‡å®šæ–‡ä»¶åŠ è½½å·¥å…·æ•°æ®"""
    if not os.path.exists(file_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        tools_data = json.load(f)
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(tools_data)} ä¸ªå·¥å…·")
    return tools_data



def validate_environment():
    """éªŒè¯ç¯å¢ƒé…ç½®"""
    
    if not os.getenv('OPENAI_API_KEY'):
        print(f"âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: OPENAI_API_KEY")
        print("è¯·ç¡®ä¿åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® OPENAI_API_KEY")
        return False
    
    print("âœ… ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡")
    return True


def display_analysis_results(analysis: dict):
    """æ˜¾ç¤ºåˆ†æç»“æœ"""
    print("\nğŸ“ˆ å·¥å…·è´¨é‡è¯„ä¼°ç»“æœåˆ†æ")
    print("="*60)
    
    print(f"ğŸ“Š åŸºç¡€ç»Ÿè®¡:")
    print(f"  æ€»å·¥å…·æ•°é‡: {analysis.get('total_count', 0)}")
    print(f"  å¹³å‡è´¨é‡åˆ†æ•°: {analysis.get('average_score', 0)}")
    print(f"  åˆ†æ•°èŒƒå›´: {analysis.get('min_score', 0)} - {analysis.get('max_score', 0)}")
    
    quality_summary = analysis.get('quality_summary', {})
    print(f"\nğŸ¯ è´¨é‡æ¦‚è§ˆ:")
    print(f"  é«˜è´¨é‡å·¥å…·æ¯”ä¾‹: {quality_summary.get('high_quality_ratio', 0)}%")
    print(f"  éœ€è¦æ”¹è¿›æ¯”ä¾‹: {quality_summary.get('needs_improvement_ratio', 0)}%")
    
    score_dist = analysis.get('score_distribution', {})
    print(f"\nğŸ“Š åˆ†æ•°åˆ†å¸ƒ:")
    print(f"  ğŸŒŸ ä¼˜ç§€ (â‰¥4.5åˆ†): {score_dist.get('excellent', 0)} ä¸ª")
    print(f"  âœ… è‰¯å¥½ (4.0-4.5åˆ†): {score_dist.get('good', 0)} ä¸ª") 
    print(f"  âš ï¸  ä¸€èˆ¬ (3.0-4.0åˆ†): {score_dist.get('average', 0)} ä¸ª")
    print(f"  âŒ è¾ƒå·® (<3.0åˆ†): {score_dist.get('poor', 0)} ä¸ª")
    
    recommendations = analysis.get('recommendations', {})
    if recommendations:
        print(f"\nğŸ’¡ æ¨èçŠ¶æ€åˆ†å¸ƒ:")
        for rec, count in recommendations.items():
            print(f"  {rec}: {count} ä¸ª")


def save_evaluation_results(evaluations: list, analysis: dict):
    """ä¿å­˜è¯„ä¼°ç»“æœ"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    tools_dir = settings.get_data_path('tools')
    file_manager = FileManager(tools_dir)
    
    # ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ
    eval_filename = f"tool_evaluations_{timestamp}.json"
    file_manager.save_json(evaluations, eval_filename)
    print(f"ğŸ’¾ è¯¦ç»†è¯„ä¼°ç»“æœå·²ä¿å­˜: {eval_filename}")
    
    # ä¿å­˜åˆ†ææŠ¥å‘Š
    analysis_filename = f"evaluation_analysis_{timestamp}.json"
    file_manager.save_json(analysis, analysis_filename)
    print(f"ğŸ’¾ åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {analysis_filename}")
    
    return eval_filename, analysis_filename


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å·¥å…·è´¨é‡è¯„ä¼°å¼€å§‹")
    print("="*60)
    
    # éªŒè¯ç¯å¢ƒ
    if not validate_environment():
        return
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logger(
        "tool_evaluation",
        level=settings.LOGGING_CONFIG["level"],
        log_file=settings.LOGGING_CONFIG["file_path"]
    )
    
    try:
        # 1. åŠ è½½æŒ‡å®šçš„å·¥å…·æ–‡ä»¶
        tools_file = "data/generated/tools/tools_batch_20250810_114153.json"
        tools_data = load_tools_from_file(tools_file)
        if not tools_data:
            print("âŒ æ— æ³•åŠ è½½å·¥å…·æ•°æ®ï¼Œç¨‹åºé€€å‡º")
            return
        
        # 2. åˆå§‹åŒ–å·¥å…·è®¾è®¡å™¨
        print("âš™ï¸ åˆå§‹åŒ–å·¥å…·è®¾è®¡å™¨...")
        tool_designer = ToolDesigner(logger=logger)
        tool_designer.initialize()
        
        print(f"ğŸ¯ å‡†å¤‡è¯„ä¼° {len(tools_data)} ä¸ªå·¥å…·")
        print(f"ğŸ”§ ä½¿ç”¨ {tool_designer.max_workers} ä¸ªçº¿ç¨‹å¹¶è¡Œå¤„ç†")
        
        # 3. æ‰¹é‡è¯„ä¼°å·¥å…·è´¨é‡
        print("\nğŸ”„ å¼€å§‹æ‰¹é‡è¯„ä¼°å·¥å…·è´¨é‡...")
        start_time = datetime.now()
        
        evaluations = tool_designer.batch_evaluate_tools(tools_data)
        
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        print(f"\nâ±ï¸ è¯„ä¼°è€—æ—¶: {processing_time:.2f} ç§’")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {len(evaluations)}/{len(tools_data)} ({len(evaluations)/len(tools_data)*100:.1f}%)")
        
        # 4. åˆ†æè¯„ä¼°ç»“æœ
        analysis = tool_designer.analyze_evaluation_results(evaluations)
        display_analysis_results(analysis)
        
        # 5. ä¿å­˜ç»“æœ
        print(f"\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
        eval_file, analysis_file = save_evaluation_results(evaluations, analysis)
        
        print(f"\nâœ… å·¥å…·è´¨é‡è¯„ä¼°å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶:")
        print(f"  - è¯¦ç»†è¯„ä¼°: {eval_file}")
        print(f"  - åˆ†ææŠ¥å‘Š: {analysis_file}")
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"å·¥å…·è¯„ä¼°å¤±è´¥: {e}")
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()