#!/usr/bin/env python3
"""
å·¥å…·è¿‡æ»¤è„šæœ¬
æ ¹æ®è´¨é‡è¯„ä¼°ç»“æœè¿‡æ»¤ä½è´¨é‡å·¥å…·ï¼Œå¹¶åŸºäºembeddingç›¸ä¼¼åº¦å»é‡
"""

import os
import sys
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from config.settings import settings
from utils.logger import setup_logger
from utils.file_manager import FileManager


def setup_filter_logger():
    """è®¾ç½®å·¥å…·è¿‡æ»¤ä¸“ç”¨æ—¥å¿—å™¨"""
    logger = setup_logger(
        "tool_filter",
        level=settings.LOGGING_CONFIG["level"],
        log_file=settings.LOGGING_CONFIG["file_path"]
    )
    return logger


def validate_environment():
    """éªŒè¯ç¯å¢ƒé…ç½®"""
    required_files = []
    
    # æ£€æŸ¥å¿…éœ€çš„ç¯å¢ƒå˜é‡
    if not os.getenv('OPENAI_API_KEY') and not os.getenv('DASHSCOPE_API_KEY'):
        print("âŒ ç¼ºå°‘APIå¯†é’¥ï¼Œéœ€è¦è®¾ç½® OPENAI_API_KEY æˆ– DASHSCOPE_API_KEY")
        return False
    
    print("âœ… ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡")
    return True


def find_latest_files():
    """æŸ¥æ‰¾æœ€æ–°çš„å·¥å…·æ–‡ä»¶å’Œè¯„ä¼°æ–‡ä»¶"""
    tools_dir = settings.get_data_path('tools')
    file_manager = FileManager(tools_dir)
    
    # æŸ¥æ‰¾å·¥å…·æ–‡ä»¶ï¼ˆä¼˜å…ˆé€‰æ‹©å¸¦embeddingçš„æ–‡ä»¶ï¼‰
    embedding_files = file_manager.list_files(".", "*tools_with_embeddings*.json")
    tools_file = None
    if embedding_files:
        tools_file = max(embedding_files, key=lambda f: file_manager.get_file_info(f)['modified'])
    
    # æŸ¥æ‰¾è¯„ä¼°æ–‡ä»¶
    evaluation_files = file_manager.list_files(".", "*tool_evaluations*.json")
    evaluation_file = None
    if evaluation_files:
        evaluation_file = max(evaluation_files, key=lambda f: file_manager.get_file_info(f)['modified'])
    
    return tools_file, evaluation_file


def load_data_files(tools_file: str, evaluation_file: str):
    """åŠ è½½å·¥å…·å’Œè¯„ä¼°æ•°æ®æ–‡ä»¶"""
    tools_dir = settings.get_data_path('tools')
    file_manager = FileManager(tools_dir)
    
    # åŠ è½½å·¥å…·æ•°æ®
    if not tools_file:
        raise FileNotFoundError("æœªæ‰¾åˆ°å·¥å…·æ•°æ®æ–‡ä»¶")
    
    print(f"ğŸ“‚ åŠ è½½å·¥å…·æ•°æ®: {os.path.basename(tools_file)}")
    tools_data = file_manager.load_json(os.path.basename(tools_file))
    print(f"âœ… æˆåŠŸåŠ è½½ {len(tools_data)} ä¸ªå·¥å…·")
    
    # åŠ è½½è¯„ä¼°æ•°æ®
    evaluations_data = []
    if evaluation_file:
        print(f"ğŸ“‚ åŠ è½½è¯„ä¼°æ•°æ®: {os.path.basename(evaluation_file)}")
        evaluations_data = file_manager.load_json(os.path.basename(evaluation_file))
        print(f"âœ… æˆåŠŸåŠ è½½ {len(evaluations_data)} ä¸ªè¯„ä¼°ç»“æœ")
    else:
        print("âš ï¸  æœªæ‰¾åˆ°è¯„ä¼°æ–‡ä»¶ï¼Œå°†è·³è¿‡è´¨é‡è¿‡æ»¤æ­¥éª¤")
    
    return tools_data, evaluations_data


def filter_tools_by_quality(tools_data: List[Dict], evaluations_data: List[Dict], 
                           quality_threshold: float = 4.0) -> Tuple[List[Dict], Dict]:
    """æ ¹æ®è´¨é‡è¯„ä¼°ç»“æœè¿‡æ»¤å·¥å…·"""
    if not evaluations_data:
        print("ğŸ“Š è·³è¿‡è´¨é‡è¿‡æ»¤æ­¥éª¤")
        return tools_data, {'skipped': True}
    
    print(f"ğŸ” æ ¹æ®è´¨é‡é˜ˆå€¼ {quality_threshold} è¿‡æ»¤å·¥å…·...")
    
    # åˆ›å»ºè¯„ä¼°ç»“æœæ˜ å°„
    evaluation_map = {}
    for eval_item in evaluations_data:
        tool_id = eval_item.get('tool_id')
        total_score = eval_item.get('overall_score', 0)
        if tool_id:
            evaluation_map[tool_id] = total_score
    
    # è¿‡æ»¤é«˜è´¨é‡å·¥å…·
    high_quality_tools = []
    quality_stats = {
        'total_tools': len(tools_data),
        'evaluated_tools': 0,
        'high_quality_tools': 0,
        'filtered_out': 0,
        'no_evaluation': 0
    }
    
    for tool in tools_data:
        tool_id = tool.get('id')
        
        if tool_id in evaluation_map:
            quality_stats['evaluated_tools'] += 1
            score = evaluation_map[tool_id]
            
            if score >= quality_threshold:
                high_quality_tools.append(tool)
                quality_stats['high_quality_tools'] += 1
            else:
                quality_stats['filtered_out'] += 1
        else:
            # æ²¡æœ‰è¯„ä¼°çš„å·¥å…·é»˜è®¤ä¿ç•™
            high_quality_tools.append(tool)
            quality_stats['no_evaluation'] += 1
    
    print(f"ğŸ“ˆ è´¨é‡è¿‡æ»¤ç»“æœ:")
    print(f"  æ€»å·¥å…·æ•°: {quality_stats['total_tools']}")
    print(f"  æœ‰è¯„ä¼°çš„å·¥å…·: {quality_stats['evaluated_tools']}")
    print(f"  é«˜è´¨é‡å·¥å…·: {quality_stats['high_quality_tools']}")
    print(f"  è¢«è¿‡æ»¤çš„å·¥å…·: {quality_stats['filtered_out']}")
    print(f"  æ— è¯„ä¼°å·¥å…·: {quality_stats['no_evaluation']}")
    
    return high_quality_tools, quality_stats


def calculate_cosine_similarity(embedding1: List[float], embedding2: List[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªembeddingå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    try:
        if not embedding1 or not embedding2:
            return 0.0
        
        if len(embedding1) != len(embedding2):
            return 0.0
        
        # æ‰‹åŠ¨è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œé¿å…ä¾èµ–sklearn
        import math
        
        # è®¡ç®—ç‚¹ç§¯
        dot_product = sum(a * b for a, b in zip(embedding1, embedding2))
        
        # è®¡ç®—å‘é‡é•¿åº¦
        norm_a = math.sqrt(sum(a * a for a in embedding1))
        norm_b = math.sqrt(sum(b * b for b in embedding2))
        
        # é¿å…é™¤é›¶
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = dot_product / (norm_a * norm_b)
        return float(max(0.0, min(1.0, similarity)))  # ç¡®ä¿ç»“æœåœ¨[0,1]èŒƒå›´å†…
        
    except Exception:
        return 0.0


def group_tools_by_scenario(tools_data: List[Dict]) -> Dict[str, List[Dict]]:
    """å°†å·¥å…·æŒ‰åœºæ™¯åˆ†ç»„"""
    scenario_groups = defaultdict(list)
    
    for tool in tools_data:
        scenario_ids = tool.get('scenario_ids', [])
        
        if scenario_ids:
            # å–ç¬¬ä¸€ä¸ªscenarioä½œä¸ºä¸»åœºæ™¯
            primary_scenario = scenario_ids[0]
            scenario_groups[primary_scenario].append(tool)
        else:
            # æ²¡æœ‰åœºæ™¯çš„å·¥å…·å•ç‹¬åˆ†ç»„
            scenario_groups['no_scenario'].append(tool)
    
    return scenario_groups


def deduplicate_tools_in_scenario(tools_in_scenario: List[Dict], 
                                similarity_threshold: float = 0.8) -> Tuple[List[Dict], Dict]:
    """åœ¨åœºæ™¯å†…åŸºäºembeddingç›¸ä¼¼åº¦å»é‡"""
    if len(tools_in_scenario) <= 1:
        return tools_in_scenario, {'clusters': 0, 'removed': 0}
    
    # è¿‡æ»¤æœ‰embeddingçš„å·¥å…·
    tools_with_embedding = []
    tools_without_embedding = []
    
    for tool in tools_in_scenario:
        embedding = tool.get('metadata', {}).get('embedding')
        if embedding and any(x != 0.0 for x in embedding):
            tools_with_embedding.append(tool)
        else:
            tools_without_embedding.append(tool)
    
    # å¦‚æœæ²¡æœ‰embeddingï¼Œç›´æ¥è¿”å›
    if len(tools_with_embedding) <= 1:
        return tools_in_scenario, {'clusters': 0, 'removed': 0}
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µå¹¶èšç±»
    clusters = []
    used_indices = set()
    
    for i, tool1 in enumerate(tools_with_embedding):
        if i in used_indices:
            continue
        
        # åˆ›å»ºæ–°ç°‡
        cluster = [i]
        embedding1 = tool1['metadata']['embedding']
        
        # å¯»æ‰¾ç›¸ä¼¼çš„å·¥å…·
        for j, tool2 in enumerate(tools_with_embedding[i+1:], i+1):
            if j in used_indices:
                continue
            
            embedding2 = tool2['metadata']['embedding']
            similarity = calculate_cosine_similarity(embedding1, embedding2)
            
            if similarity >= similarity_threshold:
                cluster.append(j)
                used_indices.add(j)
        
        clusters.append(cluster)
        used_indices.add(i)
    
    # ä»æ¯ä¸ªç°‡ä¸­é€‰æ‹©æœ€ä½³å·¥å…·
    selected_tools = []
    removed_count = 0
    
    for cluster in clusters:
        if len(cluster) == 1:
            # å•ç‹¬çš„å·¥å…·ç›´æ¥ä¿ç•™
            selected_tools.append(tools_with_embedding[cluster[0]])
        else:
            # ä»ç°‡ä¸­é€‰æ‹©æœ€ä½³å·¥å…·ï¼ˆè¿™é‡Œé€‰æ‹©ç¬¬ä¸€ä¸ªï¼Œå¯ä»¥æ ¹æ®å…¶ä»–æ ‡å‡†ä¼˜åŒ–ï¼‰
            best_tool = tools_with_embedding[cluster[0]]
            selected_tools.append(best_tool)
            removed_count += len(cluster) - 1
    
    # æ·»åŠ æ²¡æœ‰embeddingçš„å·¥å…·
    selected_tools.extend(tools_without_embedding)
    
    dedup_stats = {
        'clusters': len(clusters),
        'removed': removed_count,
        'original_count': len(tools_in_scenario),
        'final_count': len(selected_tools)
    }
    
    return selected_tools, dedup_stats


def filter_duplicate_tools(tools_data: List[Dict], similarity_threshold: float = 0.85) -> Tuple[List[Dict], Dict]:
    """åŸºäºembeddingç›¸ä¼¼åº¦åœ¨å„åœºæ™¯å†…å»é‡"""
    print(f"ğŸ”„ åŸºäºembeddingç›¸ä¼¼åº¦å»é‡ (é˜ˆå€¼: {similarity_threshold})...")
    
    # æŒ‰åœºæ™¯åˆ†ç»„
    scenario_groups = group_tools_by_scenario(tools_data)
    print(f"ğŸ“Š å‘ç° {len(scenario_groups)} ä¸ªåœºæ™¯åˆ†ç»„")
    
    # åœ¨æ¯ä¸ªåœºæ™¯å†…å»é‡
    final_tools = []
    total_stats = {
        'total_scenarios': len(scenario_groups),
        'total_clusters': 0,
        'total_removed': 0,
        'original_total': len(tools_data),
        'scenario_details': {}
    }
    
    for scenario_id, tools_in_scenario in scenario_groups.items():
        if len(tools_in_scenario) > 1:
            deduplicated_tools, dedup_stats = deduplicate_tools_in_scenario(
                tools_in_scenario, similarity_threshold
            )
            
            final_tools.extend(deduplicated_tools)
            total_stats['total_clusters'] += dedup_stats['clusters']
            total_stats['total_removed'] += dedup_stats['removed']
            total_stats['scenario_details'][scenario_id] = dedup_stats
            
            if dedup_stats['removed'] > 0:
                print(f"  åœºæ™¯ {scenario_id}: {dedup_stats['original_count']} â†’ {dedup_stats['final_count']} "
                      f"(-{dedup_stats['removed']})")
        else:
            final_tools.extend(tools_in_scenario)
            total_stats['scenario_details'][scenario_id] = {
                'clusters': 0, 'removed': 0, 
                'original_count': len(tools_in_scenario),
                'final_count': len(tools_in_scenario)
            }
    
    total_stats['final_total'] = len(final_tools)
    
    print(f"ğŸ“ˆ å»é‡ç»“æœ:")
    print(f"  åŸå§‹å·¥å…·æ•°: {total_stats['original_total']}")
    print(f"  æœ€ç»ˆå·¥å…·æ•°: {total_stats['final_total']}")
    print(f"  ç§»é™¤å·¥å…·æ•°: {total_stats['total_removed']}")
    
    return final_tools, total_stats


def save_filtered_tools(tools_data: List[Dict], quality_stats: Dict, dedup_stats: Dict):
    """ä¿å­˜è¿‡æ»¤åçš„å·¥å…·"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    tools_dir = settings.get_data_path('tools')
    file_manager = FileManager(tools_dir)
    
    # ä¿å­˜æœ€ç»ˆå·¥å…·æ•°æ®
    final_tools_file = f"final_tools_{timestamp}.json"
    file_manager.save_json(tools_data, final_tools_file)
    print(f"ğŸ’¾ æœ€ç»ˆå·¥å…·æ•°æ®å·²ä¿å­˜: {final_tools_file}")
    
    # ä¿å­˜è¿‡æ»¤ç»Ÿè®¡æŠ¥å‘Š
    filter_report = {
        'filter_summary': {
            'timestamp': timestamp,
            'final_tool_count': len(tools_data),
            'quality_filter_applied': not quality_stats.get('skipped', False),
            'similarity_deduplication_applied': True
        },
        'quality_filter_stats': quality_stats,
        'deduplication_stats': dedup_stats,
        'process_metadata': {
            'similarity_threshold': 0.8,
            'quality_threshold': 4.0,
            'processed_at': datetime.now().isoformat()
        }
    }
    
    report_file = f"filter_report_{timestamp}.json"
    file_manager.save_json(filter_report, report_file)
    print(f"ğŸ’¾ è¿‡æ»¤æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return final_tools_file, report_file


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ å·¥å…·è¿‡æ»¤å™¨")
    print("="*60)
    
    # éªŒè¯ç¯å¢ƒ
    if not validate_environment():
        return
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_filter_logger()
    
    try:
        # 1. æŸ¥æ‰¾æœ€æ–°æ–‡ä»¶
        print("ğŸ” æŸ¥æ‰¾æœ€æ–°çš„å·¥å…·å’Œè¯„ä¼°æ–‡ä»¶...")
        tools_file, evaluation_file = find_latest_files()
        
        if not tools_file:
            print("âŒ æœªæ‰¾åˆ°å·¥å…·æ•°æ®æ–‡ä»¶")
            return
        
        # 2. åŠ è½½æ•°æ®
        tools_data, evaluations_data = load_data_files(tools_file, evaluation_file)
        
        # 3. è´¨é‡è¿‡æ»¤
        filtered_tools, quality_stats = filter_tools_by_quality(
            tools_data, evaluations_data, quality_threshold=4.0
        )
        
        # 4. ç›¸ä¼¼åº¦å»é‡
        final_tools, dedup_stats = filter_duplicate_tools(
            filtered_tools, similarity_threshold=0.85
        )
        
        # 5. ä¿å­˜ç»“æœ
        print(f"\nğŸ’¾ ä¿å­˜è¿‡æ»¤ç»“æœ...")
        final_file, report_file = save_filtered_tools(final_tools, quality_stats, dedup_stats)
        
        print(f"\nâœ… å·¥å…·è¿‡æ»¤å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶:")
        print(f"  - æœ€ç»ˆå·¥å…·: {final_file}")
        print(f"  - è¿‡æ»¤æŠ¥å‘Š: {report_file}")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
    except Exception as e:
        logger.error(f"å·¥å…·è¿‡æ»¤å¤±è´¥: {e}")
        print(f"âŒ è¿‡æ»¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
